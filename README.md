# Sentiment Analysis with BERT

## Overview

This project implements sentiment analysis using BERT (Bidirectional Encoder Representations from Transformers), a state-of-the-art pre-trained language model developed by Google. The goal is to classify text into sentiment categories such as positive, negative, or neutral. 

## Features

- **Preprocessing**: Includes tokenization and padding to prepare text data for BERT.
- **BERT Model**: Utilizes BERT for feature extraction and sentiment classification.
- **Fine-Tuning**: Implements fine-tuning of the BERT model on the sentiment analysis dataset.
- **Evaluation**: Provides evaluation metrics including accuracy, precision, recall, and F1-score.

## Acknowledgements
- **BERT Model
- **Hugging Face Transformers
